%!TEX root = main.tex
\tvcg{\section{Methods} \label{methods}}
%\subsection{Evaluation Methods For Visualization Systems}
\tvcg{\subsection{Methodology Background\label{methodology_relatedwork}}}
\par Visualization systems are often evaluated using controlled studies that measure the user's performance against an existing visualization baseline~\cite{Plaisant2004}. Cognitive measures such as insight time~\cite{North2006,Yi2008} have been developed to capture how well users perform on a task against existing baselines. However, since the operations, hypotheses generated, and insights obtained through exploratory analysis are variable and subjective to individual users and their analytic goals, it is impossible to define tasks beforehand and compare across control groups. Techniques such as artificially inserting ``insights'' or setting predefined tasks for example datasets work well for objective tasks, such as debugging data errors~\cite{kandel2011wrangler,Patel2010}, but this contrived method is unsuitable for trying to learn about the types of real-world queries a user may want to pose on a VQS. \techreport{In order to make the user study more realistic, we opted for a qualitative evaluation where we allowed participants to bring a dataset that they have a vested interest in to address an unanswered research question.}
\par Due to the unrealistic nature of controlled studies, many have proposed using a more multi-faceted, ethnographic approach to understand how analysts perform visual data analysis and reasoning~\cite{Plaisant2004,lam2012empirical,shneiderman2006strategies,munzner2009nested,Sedlmair2012}. For example, multi-dimensional, in-depth, long-term case studies (MILCs) advocate the use of interviews, surveys, logging and other empirical artifacts to create a holistic understanding of how a visualization system is used in its intended environment \cite{shneiderman2006strategies}. Some papers have explored designing visualization and collaborative tools for scientific workflows through individual case studies, e.g.,~\cite{Poon2008,Chen2016}. Similarly, in our work, real-world case studies help us situate how VQSs could be used in the context of an existing analysis workflow.  
\par We adopt {\em participatory design} practices in this work: participatory design ``allows potential users to 
 participate in the design of a system that they will ultimately use''~\cite{Gould1983,Muller1993}. Participatory design has been successfully used in the development of interactive visualization systems in the past~\cite{Aragon2008,Chuang2012}. \tvcg{\cite{Sedlmair2012} highlights the benefits and pitfalls of design study. They advocate that design study methodology is suitable for use cases in which the data is available for prototyping, but the task is only partially known and information is partially in the user's head. In that regard, our scientific use cases with VQS is well-suited for a design research methodology, as we learn about the scientist's data and analysis requirements and design interactions that helps users elicit their ``in-the-head'' specifications into actionable visual queries.} %\cut{In this work, we collaborated with scientists early on to develop features in \zv that address their analysis needs.}  
\tvcg{\subsection{Our Approach}}
\par We adopted a mixed methods research methodology that draws inspiration from ethnographic methods, iterative and participatory design, and controlled studies~\cite{jorgensen_2008,miller_salkind_miller_2002,shneiderman2006strategies,Muller1993} to understand how VQSs can accelerate scientific data analysis. Our methodology served to address the research questions outlined in the introduction. Working with researchers from three different scientific research groups, we identified the needs and challenges of scientific data analysis and the potential opportunities for VQSs to fit in, via interviews and cognitive walkthroughs{\em (RQ1)}. We further extended an existing VQS, \zv, with features for scientists via participatory design{\em (RQ2)}  
\par \tvcg{We chose to build on top of \zv as functional prototype in the design study. The use of functional prototypes is common in participatory design to provide a starting point for the participants. For example, \cite{Ciolfi2016} studied two different alternatives to co-design (starting with open brief versus functional prototype) in the development of museum guidance systems and found that while both approaches were equally fruitful, functional prototypes can make addressing a specific challenge more immediate and focused (in our case the challenge is making comparison across large numbers of visualizations as we found in through informal discussions with practitioners). Our motivation for providing a functional prototype at the beginning of the participatory design sessions is to showcase capabilities of VQSs. Especially since VQSs are not uncommon in the existing workflow of these scientists, participants may not be able to imagine their use cases without a starting point.} %\sout{our prototypical VQS, since it is open-source, has both canvas-based querying capabilities as well as visualization recommendations. (\zv also has a sophisticated query language, ZQL, that we did not employ.)} 
\par As shown in Figure \ref{oldZV}, the basic version of \zv that we built on allowed users to sketch a pattern or drag-and-drop an existing visualization as a query, and \zv would then return visualizations that had the closest Euclidean distance from the queried pattern. The system also displayed representative and outlier patterns to help provide  an overview of typical trends.
	\begin{figure}[ht!]
	\centering
	\includegraphics[width=\linewidth]{figures/oldZV_nozql.png}
	\caption{The basic version of \zv that we built on top of allowed users to sketch a pattern in (a), which would then return (b) results that had the closest Euclidean distance from the sketched pattern. The system also displays (c) representative patterns obtained through K-means clustering and (d) outlier patterns to help the users gain an overview of the dataset. \tvcg{The details of the system is described in our previous work \cite{Siddiqui2017,Siddiqui2017VLDB}.}}
	\label{oldZV}
	\end{figure}
\par After incorporating desired features into \ourVQS over the period of a year, we conducted a qualitative evaluation to study how our improved VQS affected the way the users explore their data{\em (RQ3,4)} . \cut{It is interesting to note that not all of the features suggested by the participants were found to be useful during our evaluation.}
\begin{table}[h]
\centering
\vspace{-10pt}
\includegraphics[width=\linewidth]{figures/participants.pdf}
\caption{Participant information. The Likert scale used for dataset familiarity ranges from 1 (not at all familiar) to 5 (extremely familiar).}
\label{participants}
\vspace{-10pt}
\end{table}
\tvcg{\section{Understanding Scientific Data Analysis (R1)}}
%\cut{ \subsection{Understanding Scientific Data Analysis}
%\par Our initial inspiration for building a VQS came from informal discussions with many academic and industry analysts. Their current workflows required the analysts to manually examine a large number of visualizations to derive insights from their data. In this section, we address RQ1 by understanding the limitations and opportunities in existing scientific data analysis workflows in three research areas. We begin by describing the participants in these areas.} 
\subsection{Participants, Datasets and Workflows}
We recruited participants by reaching out to research groups who were interested in using VQSs for exploring their data via email.\dor{Not sure if we should make this more general about challenges in data analysis rather than focus on VQS to correspond to our earlier statement.} We summarize the common properties and differences of these three groups of researchers in Figure~\ref{example} and the desirable characteristics common to these datasets suitable for VQSs in \tvcg{the Section~\ref{metastudy}}. Six scientists from three research groups participated in the design of \zv. The evaluation study participants included these six scientists, along with three ``blank-slate'' participants who had never encountered \zv before. While participatory design subjects actively provided feedback on \zv with their data, they only saw us demonstrating their requested features and explaining the system to them, rather than actively using the system on their own. So the evaluation study was the first time that all nine of the participants used \zv to explore their datasets. We list the participants in Table~\ref{participants}, and refer to them by their anonymized ID as listed in the table. 
\techreport{On average, the participants had more than 8 years of experience working in their respective fields.}
\begin{figure*}[ht!]
\centering
\vspace{-10pt}
\includegraphics[width=6in]{figures/timeline_new.pdf}
\vspace{-6pt}\caption{Participatory design timeline for the scientific use cases.}
\label{timeline}
\vspace{-10pt}
\end{figure*}

\techreport{The research questions and objectives of the participants were diverse even among those in the same subject area and using the same dataset. Examples of research questions included: 
\begin{denselist}
\item Understanding the gene expression profiles of breast cancer cells that exhibit induced, transient, and  repressed patterns after a particular treatment.
\item Studying common patterns among stars that exhibit planetary transits versus stars that don't from the Kepler space telescope\footnote{\url{www.nasa.gov/mission_pages/kepler/main/index.html}}.
\item Identifying battery solvents with favorable properties and mass production potential through studying how changes in certain chemical properties correlate to changes in other chemical properties. \end{denselist}
}
\par The pre-study survey with the participants showed that out of all of the steps in their data analysis workflow\footnote{This includes viewing and browsing data, data cleaning and wrangling, computing statistics, data visualization, and model building or machine learning.}, they spend the most time computing statistics and creating visualizations.
The main bottlenecks cited in their existing workflow included the challenge of dealing with large amounts of data, writing custom processing and analysis scripts, and long turnaround times incurred by making modifications to an upstream operation in a segmented workflow. \ccut{On average, participants expressed interest in adopting \zv in their day-to-day workflow as eight from a Likert scale of ten after the study \kk{not sure what this means}.}
\par During the participatory design process, we collaborated with each of the teams closely with an average of two meetings per month, where we learned about their datasets, objectives, and how VQSs could help address their research questions. A detailed timeline of our engagement with the participants and the features inspired by their use cases can be found in Figure \ref{timeline}. Participants provided datasets they were exploring from their domain, whereby they had a vested interest to use a VQS to address their own research questions. We  describe the three scientific use cases below.

\par \textbf{Astronomy (\astro):} The Dark Energy Survey (DES) is a multi-institutional project with over 400 scientists. Scientists use a multi-band telescope that takes images of 300 million galaxies over 525 nights to study dark energy\cite{Drlica-Wagner2017}. The telescope also focuses on smaller patches of the sky on a weekly interval to discover astrophysical transients (objects whose brightness changes dramatically as a function of time), such as supernova explosions or quasars. The output is a time series of brightness observations associated with each object extracted from the images observed. %\kk{is it 4 years of 525 nights?}\dor{it's a bit confusing, because they only observe half a year on the southern hemisphere, will skip talking about this} 

For over five months, we worked closely with an astronomer on the project's data management team working at a supercomputing facility. \cut{We also gathered feedback from other astronomers who were interested in studying the properties of astrophysical transients during a collaboration-wide meeting.} The scientific goal is to identify a smaller set of potential candidates that may be astrophysical transients in order to study their properties in more detail. These insights can help further constrain physical models regarding the formation of these objects.
\par \textbf{Genetics (\bio):} Gene expression is a common data type used in genomics and is obtained via microarray experiments. \techreport{In these experiments, a grid containing thousands of DNA fragments are exposed to stimuli and measurements for the level at which a gene is expressed are recorded as a function of time.} The data used in the participatory design sessions was the gene expression data over time for mouse stem cells aggregated over multiple experiments, downloaded from an online database\footnote{\url{ncbi.nlm.nih.gov/geo/}}. 
\par  We worked with \tvcg{a graduate student and a PI} at a research university over three months who were using gene expression data to better understand how genes are related to phenotypes expressed during early development~\cite{Peng2016,Gloss084442}. They were interested in using \zv to cluster gene expression data before conducting analysis with a downstream machine learning workflow. 
\par \textbf{Material Science (\matsci):} We collaborated with material scientists at a research university who are working to identify solvents that can improve battery performance and stability. These scientists work with large datasets containing over 25 chemical properties for more than 280K different solvents obtained from simulations. \techreport{Once they have identified a solvent that also produces favorable results in an experiment, they identify other solvents with similar properties,  which may be cheaper or safer to manufacture at an industrial scale.}
\par We worked closely with two graduate students and a PI for over a year to design a sensible way of exploring their data using VQSs\footnote{Note that while we have interacted with the \matsci participant during the initial development stage, the participatory design formally started in June 2016}. Each row of their dataset represents a unique solvent, and consists of 25 different chemical attributes. They wanted to use \zv to identify solvents which have similar properties to known solvents but are more favorable (e.g. cheaper or safer to manufacture), and identify how changes in certain chemical attributes affects them.
\raggedbottom

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cognitive Walkthrough Sessions}
Cognitive walkthroughs highlight the existing workflows and behavior that participants have adopted for conducting certain tasks~\cite{Nielsen1994}. In our case, we observed the participants as they conducted a cognitive walkthrough demonstrating every component of their current data analysis workflow.
\techreport{
	\begin{figure}[ht!]
		\centering
		\includegraphics[width=\linewidth]{figures/workflow.pdf}
		\caption{Examples of the scientists' original workflow: a) The astronomer performs various data analysis task using the Jupyter notebook environment, b) The geneticists uses a domain-specific software to examine clustering outputs.}
		\label{workflow}
	\end{figure}
}
\par \textbf{Astronomy:} Since astronomical datasets are often terabytes in scale, they are often processed and stored in highly specialized data management centers in supercomputing centers. The collaboration's data management team has created a command-line interface that enables users to easily query, browse, and download their data\techreport{\footnote{ \url{github.com/mgckind/easyaccess}}}. 
After the data is downloaded, most of the work is done programmatically through Python in an interactive Jupyter notebook environment\footnote{\url{jupyter.org}}. The astronomer inspects the data schema, performs data cleaning and  wrangling, computes relevant statistics, and generates visualizations to search for anomalies or objects of interest, as shown in Figure \ref{workflow}a.
\par While an experienced astronomer who has examined many transient light curves can often distinguish an interesting transient object from noise by sight, they must visually examine and iterate through large numbers of visualizations of candidate objects. Manual searching is time-consuming and error prone as the majority of the objects will not be astronomical transients.
Participant A1 was interested in \zv as he recognized how specific pattern queries could help scientists directly search for these rare objects. 
\techreport{\par If an object of interest or region is identified through the visual analysis, then the astronomer may be interested in inspecting the image of the region for cross-checking that the significant change in brightness of the object is not due to an imaging artifact. This could be done using a custom built web-interface that facilitates the access of cutout images for a queried region of the sky.} 

\par \textbf{Genetics:} Participant G1 processes the raw microarray data by using a preprocessing script written in R \techreport{, where they (i) sub-select 144 genes of interest, (ii) clean up an experimental artifact due to measurements on multiple probes, (iii) log-transform the raw data to show a more distinct shape profile for clustering, (iv) normalize the gene expression values into the range of 0 to 1, and (v) perform Loess smoothing with default parameters to reduce the noise in the data}. To analyze the data, the preprocessed data is loaded into a desktop application for visualizing and clustering gene expression data\footnote{ \url{www.cs.cmu.edu/~jernst/stem/}}. G1 sets several clustering and visualization parameters on the interface before pressing a button to execute the clustering algorithm. The cluster visualizations are then displayed as overlaid time series for each cluster, as shown in the visualization in Figure \ref{workflow}b. G1 visually inspects that all the patterns in each cluster look ``clean'' and checks the number of outlier genes that do not fall into any of the clusters.  If the number of outliers is high or the visualizations look unclean, they rerun the clustering by increasing the number of clusters. Once the visualized clusters look ``good enough'', G1 exports the cluster patterns into a csv file to be used as features in their downstream regression tasks.
\par Prior to the study, the student (G1) and PI (G3) spent over a month attempting to determine the best number of clusters for their upstream analysis based on a series of static visualizations and statistics computed after clustering. While regenerating their results took no more than 15 minutes every time they made a change, the multi-step, segmented workflow meant that all changes had to be done offline, so that valuable meeting time was not wasted trying to regenerate results. The team had a vested interest in participating in the design of \zv as they saw how the interactive nature of VQSs and the ability to query other time series with clustering results could dramatically speed up their collaborative analysis process. 
\par \textbf{Material Science:} Participant M1 starts his data exploration process with a list of known and proven solvents as a reference. For instance, he would search for solvents which have boiling point over 300 Kelvins and the lithium solvation energy under 10 kcal/mol using basic SQL queries. This would help him narrow down the list of solvents, and he would continue this process with other properties. The scientist also considers the availability and the cost of the solvents while exploring the dataset. When the remaining list of the solvents is sufficiently small, he drills down to more detail (e.g., such as looking at the chemical structure of the solvents to consider the feasibility of conducting experiments with the solvent). While he had identified potential solvents through  manual lookup and comparison,  the process lacked the ability to reveal complicated trends and patterns that might be hidden, such as how the change in one attribute can affect the behavior of other attributes of a solvent. M1 was interested in using a VQS as it was infeasible for him to manually compare between large numbers of solvents and their associated properties manually.
\raggedbottom