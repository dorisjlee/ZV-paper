%!TEX root = main.tex
  \section{Methods\label{sec:methods}}
  %to understand how VQSs can be used in scientific data analysis
  \change{Via interviews and contextual inquiries in participants' normal work environments, we first identified the needs and challenges in participants' existing data analysis workflows. Given these challenges, we collaboratively designed VQS functionalities by engaging with experts from three different domains in the process of participatory design, leading to a final prototype \zvpp. At the end of the design study, we conducted a realistic evaluation study to study how VQSs are used in the real-world analytical workflows.}
  \subsection{Phase I: \change{Contextual Inquiry and }Participatory Design}
  \par We recruited participants by reaching out to research groups, who have experienced challenges in data exploration, via email and word of mouth. Based on our early conversations with analysts from 12 different potential application areas, we narrowed down to three use cases in astronomy, genetics, and material science for our design study, chosen based on their suitability for VQSs as well as diversity in use cases. Six scientists (1 female, 5 male), with an average of more than 6 years of research experience in their respective fields, participated in the design process. Via interviews and contextual inquiries, we identified the needs and challenges of these use cases based on each participant's existing analysis workflow.\change{ We discuss these findings in Section~\ref{sec:participantdatasets}}
  %\par For the participatory design study, we built on an existing VQS, \zv~\cite{Siddiqui2017,Siddiqui2017VLDB}, that allowed users to sketch a pattern or drag-and-drop an existing visualization as a query, with the system returning visualizations that had the closest Euclidean distance to the queried pattern. We chose to build on top of \zv, since it was open-source, extensible, and included features beyond pattern and match specification typically found in existing systems (as compared in Table~\ref{table:relatedwork}) and described in Section~\ref{sec:sensemaking}. Past research on participatory design has found that the use of functional prototypes is a common and effective way of engaging with participants and providing a starting point for participatory design~\cite{Ciolfi2016}. Our motivation for providing a functional prototype at the beginning of the participatory design sessions was to showcase capabilities of VQSs. Since our participants were not aware of the existence of VQSs, let alone using them in their workflows, they would not have been able to imagine use cases for VQS without a starting point.%VQSs are not common in the existing workflows of these scientists, participants may not be
  \par \change{For the participatory design study, we built on top of an existing VQS, \zv~\cite{Siddiqui2017,Siddiqui2017VLDB}, to create a functional prototype that served as a starting point for discussion around VQSs.} During participatory design, we collaborated with each team closely with an average of two \change{1-hour-long} meetings per month, where we learned about their datasets, objectives, and what additional VQS functionalities could help address their research questions. 
  \par Since some of the essential features that were crucial for effective exploration were lacking in \zv and still under development in the new version of our VQS, \zvpp, we did not provide a deployed prototype for participants to actively use on their own during the participatory design period. Instead, as we iterated on the design of these features, relevant capabilities from intermediate versions of \zvpp were demonstrated to the participants. \change{As ``\textit{simulated future work situations}'' are common in participatory design,} participants also had the opportunity to interact with the low-fidelity prototype through the help of a guided facilitator. %Such use of ``\textit{simulated future work situation}'' is common in cooperative prototyping when the real use of the prototype is not feasible~\cite{Grnbak1991}. 
  During this process, we elicited feedback from participants on how the VQS could better support their scientific goals. A summary timeline of our collaboration with participants over a year can be found in Figure \ref{timeline} in \change{Appendix \ref{apdx:pdartifact}}. %During these sessions, we discussed and iterated on the design of additional features for \zvpp.
  %Participants provided datasets they were exploring from their domain, whereby they had a vested interest in using a VQS to address their own research questions.
  %into the new version of our VQS,
  Through this process, we identified and incorporated several crucial capabilities into \zvpp\change{, as listed in Table~\ref{bigfeaturetable}}. % , described more in Section~\ref{sec:pd_findings}.
  \begin{table}[hb!]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/participant_info.pdf}
    \caption{Participant information. The Likert scale used for dataset familiarity ranges from 1 (not familiar) to 5 (extremely familiar).}
    \label{participants}
    \vspace*{-10pt}
  \end{table}
  \subsection{Phase II: Evaluation Study}
  % \par Visualization systems are often evaluated using controlled studies that measure the user's performance against an existing visualization baseline~\cite{Plaisant2004}. Techniques such as artificially inserting ``insights'' or setting predefined tasks for example datasets work well for objective tasks, \techreport{such as debugging data errors~\cite{kandel2011wrangler,Patel2010},} but they are unsuitable for trying to learn about the types of real-world queries users may want to pose on VQSs. %Due to the unrealistic nature of controlled studies, many have proposed using a more multi-faceted, ethnographic approach to understand how analysts perform visual data analysis and reasoning~\cite{Plaisant2004,lam2012empirical,shneiderman2006strategies,munzner2009nested,Sedlmair2012}.
  %In order to make the evaluation more realistic, we invited p
  At the end of our participatory design study, we performed a qualitative evaluation to study how analysts interact with different VQS components in practice. \change{Participants used }datasets that they have a vested interest in exploring to address unanswered research questions \change{(a total of six different datasets across nine participants)}. As shown in Table~\ref{participants}, the evaluation study participants included the six scientists from participatory design, along with three additional ``blank-slate'' participants who had never encountered \zvpp before. The use of all or a subset of the project \change{stakeholders} as evaluation participants is common in participatory design~\cite{Bossen2016}. \change{While the small sample size of participants is a limitation, this is a common challenge when recruiting domain-experts, whose specific expertise and skills are rare and have limited time due to their workplace demands relative to the general population, as echoed in prior work\cite{Batch2018,Mclachlan2008}.} %\dor{Addressing Aditya's comments, we can't exactly say that most other PD work involves significantly fewer participants. For example, LiveRAC involved 14 participants in 3 groups; Batch and Elmqvist had  8 participants. So our study is about typical in size.}
  %As detailed in Table~\ref{participants}, the nine participants brought a total of six different datasets to the study. 
  %and ---- to participate in studies
  %(e.g., stakeholders)
   %\cut{Since the participatory design subjects acted as informants and did not actively try out the system on their own, the evaluation study was the first time that all participants used \zvpp to explore their datasets.}
  %\ccut{While participatory design subjects actively provided feedback on \zvpp with their data, they only saw us demonstrating their requested features and explaining the system to them, rather than actively using the system on their own. So}
  % \dor{We could summarized the next two paragraph as simply participants engaged in talk-aloud excercise while exploring their datasets using \zvpp...Replacing everything until "The user study ended after ...". But if we are not short on space, we can keep the existing description.}
  \par Evaluation study participants were recruited from each of the three aforementioned research groups, as well as domain-specific mailing lists. Prior to the study, we asked potential participants to fill out a pre-study survey to determine eligibility. Eligibility criteria included: being an active researcher in the subject area with more than one year of experience, and having worked on a research project involving data of the same nature used in participatory design. None of the participants received monetary compensation for the study, as this is not a common practice for participatory design with stakeholders~\cite{Ommen2016,McNally2017}. \techreport{The research questions and objectives of the participants were diverse even among the same subject area. Examples included understanding gene expression profiles of breast cancer cells after a particular treatment and comparing common patterns among stars that exhibit planetary transits versus stars that do not.\techreport{from the Kepler space telescope\footnote{\url{www.nasa.gov/mission_pages/kepler/main/index.html}}.}}
  \par At the start, participants were provided with an interactive walk-through of \change{\zvpp and given approximately ten minutes for a guided exploration of }a preloaded real-estate example dataset. \techreport{from Zillow \cite{zillow}. This dataset contained housing data for various cities, metropolitan areas, and states in the U.S. from 2004-15.}After familiarizing themselves with the tool, we loaded the participant's dataset and encouraged them to talk-aloud during data exploration, and use external tools or other resources as needed. If the participant was out of ideas\ccut{ for three minutes}, we suggested one of the main VQS functionalities \footnote{query by sketching, drag-and-drop, pattern loading, input equations, representative and outliers, narrow/ignore x-range options, filtering, data smoothing, creating dynamic classes,  data export} that they had not yet used. If any of these operations were not applicable to their specific dataset, they were allowed to skip the operation after having considered \change{it}. The user study ended after they covered all the main functionalities and lasted on average for 63 minutes. After the study, we asked participants open-ended questions about their experience.